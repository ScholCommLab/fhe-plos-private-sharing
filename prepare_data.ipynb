{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Preprocessing of data for: How much research shared on Facebook is hidden from public view?\n",
    "\n",
    "1. Load required data and provide some basic stats\n",
    "2. Remove all articles that have been wrongly aggregated by Facebook\n",
    "3. Process collected metrics\n",
    "4. Match articles with disciplinary information\n",
    "5. Write output files used in analysis notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load required data and provide some basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "disciplines_csv = \"data/external/PLOS_2015-2017_idArt-DOI-PY-Journal-Title-LargerDiscipline-Discipline-Specialty.csv\"\n",
    "\n",
    "in_articles_csv = \"data/input/plos_one_articles.csv\"\n",
    "details_csv = \"data/input/query_details.csv\"\n",
    "fb_metrics_csv = \"data/input/graph_api_counts.csv\"\n",
    "am_metrics_csv = \"data/input/altmetric_counts.csv\"\n",
    "\n",
    "# Output data\n",
    "out_articles_csv = \"data/articles.csv\"\n",
    "out_responses_csv = \"data/responses.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load articles and extract years\n",
    "all_articles = pd.read_csv(in_articles_csv, index_col=\"doi\", parse_dates=['publication_date'])\n",
    "all_articles['year'] = all_articles.publication_date.map(lambda x: x.year)\n",
    "\n",
    "# Replace authors of articles by PLOS without author information with \"PLOS ONE\"\n",
    "all_articles.loc[all_articles[all_articles.author.isna()].index, \"author\"] = \"PLOS ONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load responses\n",
    "all_responses = pd.read_csv(details_csv, index_col=\"id\", parse_dates=['received_at', 'og_updated_time', 'publication_date', 'added_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both metrics files and merge\n",
    "fb_metrics = pd.read_csv(fb_metrics_csv, index_col=\"doi\")\n",
    "am_metrics = pd.read_csv(am_metrics_csv, index_col=\"doi\")\n",
    "all_metrics = fb_metrics.join(am_metrics, how=\"outer\")\n",
    "\n",
    "# Rename the metrics\n",
    "col_names = {\n",
    "    'twitter': 'TW_og',\n",
    "    'facebook': 'POS_og',\n",
    "    'shares': 'AES_og',\n",
    "    'reactions': 'AER_og',\n",
    "    'comments' : 'AEC_og'\n",
    "}\n",
    "all_metrics.rename(columns=col_names, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load disciplines\n",
    "disciplines = pd.read_csv(disciplines_csv, delimiter=\";\", index_col=\"DOI\")\n",
    "disciplines.index = disciplines.index.map(lambda x: str(x)[4:])\n",
    "\n",
    "# Rename columns\n",
    "col_names = {\n",
    "    \"EGrande_Discipline\": \"grand_discipline\",\n",
    "    \"EDiscipline\": \"discipline\",\n",
    "    \"ESpecialite\": \"specialty\"\n",
    "}\n",
    "disciplines.rename(columns=col_names, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "**Some basic stats about the data before processing steps were applied:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Altmetrics results - responses: 61872\n",
      "Altmetric results - non-zero responses 43833\n",
      "Altmetric results -  at least one POS: 9632\n",
      "Altmetric results - at least one TW: 43083\n"
     ]
    }
   ],
   "source": [
    "print(\"Altmetrics results - responses:\", am_metrics.shape[0])\n",
    "print(\"Altmetric results - non-zero responses\", am_metrics.dropna(how=\"all\").shape[0])\n",
    "print(\"Altmetric results -  at least one POS:\", am_metrics.facebook.replace(0, np.nan).count())\n",
    "print(\"Altmetric results - at least one TW:\", am_metrics.twitter.replace(0, np.nan).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB responses - responses: 49121\n",
      "FB responses - non-zero responses 49121\n",
      "FB responses with at least one share: 21439\n",
      "FB responses with at least one reaction: 13849\n",
      "FB responses with at least one comment: 10283\n",
      "FB responses with at least one plugin comment: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"FB responses - responses:\", fb_metrics.shape[0])\n",
    "print(\"FB responses - non-zero responses\", fb_metrics.dropna(how=\"all\").shape[0])\n",
    "print(\"FB responses with at least one share:\", fb_metrics.shares.replace(0, np.nan).count())\n",
    "print(\"FB responses with at least one reaction:\", fb_metrics.reactions.replace(0, np.nan).count())\n",
    "print(\"FB responses with at least one comment:\", fb_metrics.comments.replace(0, np.nan).count())\n",
    "print(\"FB responses with at least one plugin comment:\", fb_metrics.plugin_comments.replace(0, np.nan).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles with at least one share, reaction, or comment on Facebook: 21698\n"
     ]
    }
   ],
   "source": [
    "n = len(all_metrics.replace(0, np.nan)[['AES_og', 'AER_og', 'AEC_og']].dropna(how=\"all\"))\n",
    "print(\"Articles with at least one share, reaction, or comment on Facebook: {}\".format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB queries that returned results: 69983 (11.31%)\n",
      "Responses with no engagement at all: 37062 (52.96%)\n"
     ]
    }
   ],
   "source": [
    "n_responses = all_responses[['reactions', 'shares', 'comments']].shape[0]\n",
    "print(\"FB queries that returned results: {} ({:.2f}%)\".format(n_responses, 100 * n_responses / all_articles.shape[0] / 10))\n",
    "\n",
    "zero_eng = sum(all_responses[['reactions', 'shares', 'comments']].sum(axis=1)==0)\n",
    "print(\"Responses with no engagement at all: {} ({:.2f}%)\".format(zero_eng, 100*zero_eng/n_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FB queries with results: 69983\n",
      "Found articles: 61872\n",
      "Found articles with metrics: 61872\n"
     ]
    }
   ],
   "source": [
    "print(\"FB queries with results:\", all_responses.shape[0])\n",
    "print(\"Found articles:\", all_articles.shape[0])\n",
    "print(\"Found articles with metrics:\", all_metrics.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Remove all articles that have been wrongly aggregated by Facebook\n",
    "\n",
    "The following steps removes articles that were wrongly aggregated within the Facebook social graph. See Enkhbayar and Alperin (2018) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>All articles</th>\n",
       "      <th>Dropped</th>\n",
       "      <th>Final article count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>25436</td>\n",
       "      <td>9</td>\n",
       "      <td>25427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>18815</td>\n",
       "      <td>6</td>\n",
       "      <td>18809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>17621</td>\n",
       "      <td>9</td>\n",
       "      <td>17612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All years</th>\n",
       "      <td>61872</td>\n",
       "      <td>24</td>\n",
       "      <td>61848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           All articles  Dropped  Final article count\n",
       "                                                     \n",
       "2015              25436        9                25427\n",
       "2016              18815        6                18809\n",
       "2017              17621        9                17612\n",
       "All years         61872       24                61848"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ogid_counts = all_responses.groupby([\"doi\", \"og_id\"]).size().groupby(['og_id']).count()\n",
    "\n",
    "bad_ogids = ogid_counts[ogid_counts>1].keys()\n",
    "bad_dois = all_responses[all_responses.og_id.isin(bad_ogids)].doi\n",
    "\n",
    "responses = all_responses[~all_responses.doi.isin(bad_dois)]\n",
    "articles = all_articles.drop(bad_dois, axis=0)\n",
    "metrics = all_metrics.drop(bad_dois, axis=0)\n",
    "\n",
    "dropped_years = all_articles.reindex(bad_dois).year.value_counts()\n",
    "\n",
    "# Article counts - base dataset\n",
    "n_articles_by_year = articles.groupby(\"year\").count()['title']\n",
    "n_all_articles_by_year = all_articles.groupby(\"year\").count()['title']\n",
    "\n",
    "df_article_counts = pd.DataFrame({\n",
    "    'All articles': n_all_articles_by_year,\n",
    "    'Dropped': dropped_years,\n",
    "    'Final article count': n_articles_by_year\n",
    "})\n",
    "df_article_counts.loc['All years'] = df_article_counts.sum(axis=0)\n",
    "df_article_counts.index.name = \"\"\n",
    "\n",
    "df_article_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process collected metrics\n",
    "\n",
    "Replace zero counts with NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27423 articles with 0/0/0 AE\n"
     ]
    }
   ],
   "source": [
    "# Replace any fb metric of 0 with nan\n",
    "for _ in ['AES', 'AER', 'AEC', \"POS\", \"TW\"]:\n",
    "    all_metrics[_] = all_metrics[_+\"_og\"][all_metrics[_+\"_og\"] != 0]\n",
    "    \n",
    "x = all_metrics[['AES_og', 'AEC_og', 'AER_og']] == 0\n",
    "print(\"{} articles with 0/0/0 AE\".format(x.all(axis=1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metrics to articles\n",
    "articles = articles.join(all_metrics[[\"AES\", \"POS\", \"TW\", \"AER\", \"AEC\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Match articles with disciplinary information\n",
    "\n",
    "Disciplinary information for each article is provided by Piwowar et al. (2018). In order to use the data the articles were matched with several steps:\n",
    "\n",
    "1. Match articles by DOIs\n",
    "2. Match articles by titles (after conversion to alphanum & lowercase)\n",
    "\n",
    "In 6 cases the disciplinary information provided multiple disciplines for articles in which we chose one randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert titles to alphanum & lowercase for in both datasets\n",
    "articles['title_'] = articles['title'].map(lambda x: ''.join(e for e in x.lower() if e.isalnum()))\n",
    "disciplines['title_'] = disciplines['title'].map(lambda x: ''.join(e for e in x.lower() if e.isalnum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 articles with multiple disciplines. Selecting one randomly.\n"
     ]
    }
   ],
   "source": [
    "# Naive join of articles and disciplinary information by DOIs\n",
    "x = articles.join(disciplines[[\"grand_discipline\", \"discipline\", \"specialty\"]], how=\"left\")\n",
    "\n",
    "# Articles with multiple disciplines\n",
    "print(x.index.duplicated().sum(), \"articles with multiple disciplines. Selecting one randomly.\")\n",
    "\n",
    "# Dropping of the duplicate disciplines randomly\n",
    "x = x[~x.index.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing articles after DOI matching: 7989\n"
     ]
    }
   ],
   "source": [
    "# select those that still miss disciplines\n",
    "missings = x[x.discipline.isna()].copy()\n",
    "missings = missings.drop([\"grand_discipline\", \"discipline\", \"specialty\"], axis=1)\n",
    "print(\"Missing articles after DOI matching:\", missings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title matching found: 4060\n",
      "Missing articles after title matching: 3929\n"
     ]
    }
   ],
   "source": [
    "# try to match these with titles and replace in x\n",
    "found = missings.reset_index().merge(disciplines[[\"grand_discipline\", \"discipline\", \"specialty\", \"title_\"]], left_on=\"title_\", right_on=\"title_\", how=\"inner\").set_index('index')\n",
    "print(\"Title matching found:\", found.shape[0])\n",
    "x.loc[found.index] = found\n",
    "\n",
    "# select those that still miss disciplines\n",
    "missings = x[x.discipline.isna()].copy()\n",
    "missings = missings.drop([\"grand_discipline\", \"discipline\", \"specialty\"], axis=1)\n",
    "print(\"Missing articles after title matching:\", missings.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop temp column with modified titles\n",
    "articles = x.drop(columns=\"title_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artices by PLOS 1197\n",
      "Corrections 3332\n",
      "Retractions 36\n"
     ]
    }
   ],
   "source": [
    "author_plos = articles.author.str.contains(\"PLOS\")\n",
    "type_corr = articles.title.str.contains(\"Correction: \")\n",
    "type_retr = articles.title.str.contains(\"Retraction: \")\n",
    "\n",
    "print(\"Artices by PLOS\", author_plos.sum())\n",
    "print(\"Corrections\", type_corr.sum())\n",
    "print(\"Retractions\", type_retr.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Articles with disciplines</th>\n",
       "      <td>57919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Articles not in disc. dataset</th>\n",
       "      <td>3374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual missing articles</th>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sum</th>\n",
       "      <td>61848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Count\n",
       "Articles with disciplines      57919\n",
       "Articles not in disc. dataset   3374\n",
       "Actual missing articles          555\n",
       "Sum                            61848"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"Count\"])\n",
    "df.loc['Articles with disciplines'] = sum(~x.discipline.isna())\n",
    "df.loc['Articles not in disc. dataset'] = sum([any(x) for x in zip(author_plos, type_corr, type_retr)])\n",
    "df.loc['Actual missing articles'] = x.discipline.isna().sum()-sum([any(x) for x in zip(author_plos, type_corr, type_retr)])\n",
    "df.loc['Sum'] = df.sum()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write output files used in analysis notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.index.name = \"doi\"\n",
    "\n",
    "articles.to_csv(\"data/articles.csv\")\n",
    "responses.to_csv(\"data/responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# References\n",
    "\n",
    "Enkhbayar, A., & Alperin, J. P. (2018). Challenges of capturing engagement on Facebook for Altmetrics. STI 2018 Conference Proceedings, 1460–1469. Retrieved from http://arxiv.org/abs/1809.01194\n",
    "\n",
    "Piwowar, H., Priem, J., Larivière, V., Alperin, J. P., Matthias, L., Norlander, B., … Haustein, S. (2018). The state of OA: A large-scale analysis of the prevalence and impact of Open Access articles. PeerJ, 6, e4375. doi: [10/ckh5](https://doi.org/10/ckh5)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "altmetrics",
   "language": "python",
   "name": "altmetrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
